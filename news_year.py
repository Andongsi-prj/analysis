import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pymysql
import re
import time
import random

# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL (ìµœì‹ ìˆœ ì •ë ¬)
base_url = "https://search.naver.com/search.naver?where=news&query=ì² ê°•&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds={}&de={}&start={}"

# âœ… MySQL ì—°ê²° ì„¤ì •
db = pymysql.connect(
    host="192.168.0.163",
    user="analysis_user",
    password="andong1234",
    database="analysis",
    charset="utf8mb4"
)

cursor = db.cursor()

# âœ… HTTP ì„¸ì…˜ ì„¤ì • ë° ì¬ì‹œë„ ì „ëµ ì ìš©
session = requests.Session()
retry_strategy = Retry(total=3, backoff_factor=2, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("https://", adapter)
session.mount("http://", adapter)

# âœ… ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ í—¤ë” ì„¤ì •
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Referer": "https://www.naver.com/",
    "Accept-Language": "ko-KR,ko;q=0.9",
    "Cookie": "NNB=your_cookie_here;"  # ë„¤ì´ë²„ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¨ ì¿ í‚¤ ê°’ ì¶”ê°€
}

# âœ… ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ì œëª© ì €ì¥ (ì´ë¯¸ í¬ë¡¤ë§í•œ ê¸°ì‚¬ ì²´í¬)
seen_titles = set()

def get_news_list(url, counter, last_known_date):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‰´ìŠ¤ ì œëª©, ë³¸ë¬¸ ì „ì²´, ë³´ë„ ë‚ ì§œ í¬ë¡¤ë§ í›„ MySQLì— ì €ì¥"""
    try:
        response = session.get(url, headers=headers)
        if response.status_code == 403:
            print("âŒ 403 Forbidden - ë„¤ì´ë²„ ì°¨ë‹¨ë¨. ì¼ì • ì‹œê°„ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            time.sleep(600)
            return False, last_known_date

        response.raise_for_status()
        random_sleep()
    except requests.RequestException as e:
        print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
        return False, last_known_date

    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.select(".news_area")

    if not articles:
        return False, last_known_date

    for article in articles:
        try:
            # âœ… ê¸°ì‚¬ ì œëª© í¬ë¡¤ë§
            title_element = article.select_one("a.news_tit")
            title = title_element.text.strip() if title_element else "ì œëª© ì—†ìŒ"
            article_url = title_element["href"] if title_element else None  # ê¸°ì‚¬ URL ê°€ì ¸ì˜¤ê¸°

            # âœ… ì¤‘ë³µ ê¸°ì‚¬ ë°©ì§€
            if title in seen_titles:
                continue
            seen_titles.add(title)

            # âœ… ë³´ë„ ë‚ ì§œ í¬ë¡¤ë§
            publish_date_element = article.select("span.info")
            publish_date = "ì•Œ ìˆ˜ ì—†ìŒ"
            for elem in publish_date_element:
                date_text = elem.get_text(strip=True)
                publish_date = parse_publish_date(date_text)
                break

            # âœ… ë‚ ì§œ í˜•ì‹ ë³€ê²½ (YYYY-MM-DD)
            if publish_date == "ì•Œ ìˆ˜ ì—†ìŒ":
                publish_date = last_known_date
            else:
                last_known_date = publish_date

            # âœ… ë³¸ë¬¸ í¬ë¡¤ë§ (ì˜¬ë°”ë¥¸ ì„ íƒì ì ìš©)
            summary_element = article.select_one("div.news_contents > div > div > a")
            content = summary_element.text.strip() if summary_element else "ë³¸ë¬¸ ì—†ìŒ"

            # âœ… ë°ì´í„° MySQLì— ì €ì¥
            save_to_db(title, content, publish_date)

            # âœ… ë‚ ì§œì™€ ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜ë§Œ ì¶œë ¥ (ì‹œë¶„ì´ˆ ì œê±°)
            counter[0] += 1
            formatted_date = publish_date.split(" ")[0]  # âœ… 'YYYY-MM-DD' ë¶€ë¶„ë§Œ ì¶”ì¶œ
            print(f"âœ… {formatted_date} - ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜: {counter[0]}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return True, last_known_date

def parse_publish_date(date_text):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¨ ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ ë³€í™˜ (YYYY-MM-DD HH:MM:SS)"""
    current_time = datetime.now()

    match_minute = re.search(r"(\d+)ë¶„ ì „", date_text)
    if match_minute:
        minutes_ago = int(match_minute.group(1))
        return (current_time - timedelta(minutes=minutes_ago)).strftime("%Y-%m-%d %H:%M:%S")

    match_hour = re.search(r"(\d+)ì‹œê°„ ì „", date_text)
    if match_hour:
        hours_ago = int(match_hour.group(1))
        return (current_time - timedelta(hours=hours_ago)).strftime("%Y-%m-%d %H:%M:%S")

    match_day = re.search(r"(\d+)ì¼ ì „", date_text)
    if match_day:
        days_ago = int(match_day.group(1))
        return (current_time - timedelta(days=days_ago)).strftime("%Y-%m-%d")

    match_date = re.search(r"(\d{4})[.](\d{2})[.](\d{2})[.]", date_text)
    if match_date:
        return f"{match_date.group(1)}-{match_date.group(2)}-{match_date.group(3)}"

    return "ì•Œ ìˆ˜ ì—†ìŒ"

def save_to_db(title, content, news_date):
    """MySQLì— ë‰´ìŠ¤ ë°ì´í„° ì €ì¥"""
    try:
        insert_query = """
        INSERT INTO stage_oneyear_news (title, content, news_date)
        VALUES (%s, %s, %s)
        """
        cursor.execute(insert_query, (title, content, news_date))
        db.commit()
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
        db.rollback()

def random_sleep():
    """ëœë¤í•œ ì‹œê°„ ë™ì•ˆ ëŒ€ê¸° (2~6ì´ˆ, 20% í™•ë¥ ë¡œ 10~15ì´ˆ)"""
    sleep_time = random.uniform(3, 4)
    if random.random() < 0.2:
        sleep_time = random.uniform(5, 7)
    
    print(f"â³ {sleep_time:.2f}ì´ˆ ëŒ€ê¸°...")
    time.sleep(sleep_time)

if __name__ == "__main__":
    counter = [0] 
    last_known_date = None  

    # âœ… 2025-02-12ë¶€í„° 2024-01-01ê¹Œì§€ í¬ë¡¤ë§ (ìµœì‹  â†’ ê³¼ê±°)
    start_date = datetime(2025, 2, 12)  
    end_date = datetime(2024, 1, 1)  

    while start_date >= end_date:  # âœ… ë‚ ì§œë¥¼ ê±°ê¾¸ë¡œ ì´ë™ (ìµœì‹  â†’ ê³¼ê±°)
        prev_date = start_date - timedelta(days=2)
        if prev_date < end_date:
            prev_date = end_date

        ds = prev_date.strftime("%Y.%m.%d")
        de = start_date.strftime("%Y.%m.%d")

        print(f"ğŸ” {ds} ~ {de} ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")

        page = 1
        max_pages = 1000  

        while page <= max_pages:
            search_url = base_url.format(ds, de, (page - 1) * 10 + 1)
            prev_count = counter[0]
            success, last_known_date = get_news_list(search_url, counter, last_known_date)

            if not success or prev_count == counter[0]:  
                break
            page += 1  

        start_date = prev_date - timedelta(days=1)

    cursor.close()
    db.close()
