import requests
import pandas as pd
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import os
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# CSV íŒŒì¼ëª…
csv_filename = "steel_news.csv"

# ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë° ê¸°ê°„ ì„¤ì •
query = "ì² ê°•"
days = 2  # ìµœê·¼ 2ì¼ê°„ì˜ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘

# ë‚ ì§œ ê¸°ì¤€ ì„¤ì • (ì˜¤ëŠ˜ ë‚ ì§œì—ì„œ 2ì¼ ì „ê¹Œì§€ í—ˆìš©)
start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')

# ìµœì‹  ë¸Œë¼ìš°ì € User-Agent ì‚¬ìš©
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Referer": "https://www.naver.com/",
    "Accept-Language": "ko-KR,ko;q=0.9",
}

def get_news_links(query):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìµœê·¼ 2ì¼ê°„ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬, ì œëª©, ë‚ ì§œë¥¼ ìˆ˜ì§‘
    """
    news_data = []
    seen_links = set()
    page = 1  # í˜ì´ì§€ ì‹œì‘ ê°’

    while True:  # ğŸ”¹ í˜ì´ì§€ ì œí•œ ì—†ì´ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì¢…ë£Œ
        search_url = f"https://search.naver.com/search.naver?where=news&query={query}&sort=sim&nso=so:r,p:2d,a:all&start={page}"
        print(f"ğŸ” [í˜ì´ì§€ {page // 10 + 1}] ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘: {search_url}")

        try:
            response = requests.get(search_url, headers=headers, timeout=10)

            # 403 Forbidden ì—ëŸ¬ ë°©ì§€
            if response.status_code == 403:
                print("â›” [ì°¨ë‹¨ ê°ì§€] 403 Forbidden - User-Agent ë³€ê²½ & VPN ì‚¬ìš© ì¶”ì²œ")
                time.sleep(10)
                continue
            elif response.status_code != 200:
                print(f"âŒ [ì˜¤ë¥˜] í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ - ìƒíƒœ ì½”ë“œ: {response.status_code}")
                break

            soup = BeautifulSoup(response.text, "html.parser")
            articles = soup.select("ul.list_news li.bx")

            if not articles:
                print("âœ… ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                break  # ğŸ”¹ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ì¢…ë£Œ

            for article in articles:
                title_tag = article.select_one("a.news_tit")
                date_tag = article.select("div.info_group span.info")
                link = title_tag["href"] if title_tag else ""
                title = title_tag["title"] if title_tag else "ì œëª© ì—†ìŒ"

                # ìœ íš¨í•œ URLì¸ì§€ í™•ì¸
                if not link.startswith("http"):
                    continue

                if link in seen_links:
                    continue
                seen_links.add(link)

                news_date = "ë‚ ì§œ ì—†ìŒ"
                if date_tag:
                    for tag in date_tag:
                        text = tag.get_text()
                        if "ì „" in text:
                            news_date = convert_relative_date(text)
                            break

                # **âœ… ë‚ ì§œ ê²€ì¦ ì¶”ê°€: 2ì¼ ì´ˆê³¼í•˜ë©´ í¬ë¡¤ë§ ì¤‘ë‹¨**
                if news_date < start_date:
                    print(f"â¹ [ì¤‘ë‹¨] {news_date} ê¸°ì‚¬ëŠ” {start_date} ì´ì „ ë‰´ìŠ¤ì´ë¯€ë¡œ í¬ë¡¤ë§ ì¢…ë£Œ.")
                    return news_data  # ğŸ”¹ ìµœì‹  ë‰´ìŠ¤ê°€ ì•„ë‹ˆë©´ í¬ë¡¤ë§ ì¢…ë£Œ

                if link:
                    news_data.append({"title": title, "link": link, "news_date": news_date})

            page += 10  # ğŸ”¹ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            time.sleep(random.uniform(2, 5))  # ğŸ”¹ ëœë¤ ë”œë ˆì´ ì¶”ê°€í•˜ì—¬ íƒì§€ ë°©ì§€

        except Exception as e:
            print(f"âŒ [ì˜¤ë¥˜] ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            break

    print(f"âœ… ì´ {len(news_data)}ê°œì˜ ë‰´ìŠ¤ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    return news_data

def convert_relative_date(relative_date):
    """
    "1ì¼ ì „", "2ì‹œê°„ ì „" ê°™ì€ ìƒëŒ€ì  ë‚ ì§œë¥¼ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜
    """
    now = datetime.now()
    if "ì¼ ì „" in relative_date:
        days_ago = int(relative_date.replace("ì¼ ì „", "").strip())
        return (now - timedelta(days=days_ago)).strftime('%Y-%m-%d')
    elif "ì‹œê°„ ì „" in relative_date:
        hours_ago = int(relative_date.replace("ì‹œê°„ ì „", "").strip())
        return (now - timedelta(hours=hours_ago)).strftime('%Y-%m-%d')
    elif "ë¶„ ì „" in relative_date:
        minutes_ago = int(relative_date.replace("ë¶„ ì „", "").strip())
        return (now - timedelta(minutes=minutes_ago)).strftime('%Y-%m-%d')
    return now.strftime('%Y-%m-%d')

def save_to_csv(data, filename):
    """
    ë°ì´í„°ë¥¼ CSV íŒŒì¼ì— ì‹¤ì‹œê°„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    df = pd.DataFrame(data, columns=["title", "content", "news_date"])
    file_exists = os.path.isfile(filename)
    df.to_csv(filename, mode='a', header=not file_exists, index=False, encoding="utf-8-sig")

def crawl_naver_news():
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‰´ìŠ¤ ì œëª©, ë³¸ë¬¸, ë‚ ì§œ í¬ë¡¤ë§ í›„ CSV ì €ì¥ (ë©€í‹°ì“°ë ˆë”© ì ìš©)
    """
    news_data = get_news_links(query)

    collected_news = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(get_news_links, news): news for news in news_data}

        for future in as_completed(futures):
            try:
                news_content = future.result()
                collected_news.append(news_content)
                save_to_csv([news_content], csv_filename)
                print(f"âœ… [ì €ì¥] {news_content['title']}")

            except Exception as e:
                print(f"âŒ [ì˜¤ë¥˜] ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    print(f"âœ… [ì™„ë£Œ] {len(collected_news)}ê°œì˜ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ëë‚¬ìŠµë‹ˆë‹¤. CSV íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”! ğŸš€")

if __name__ == "__main__":
    crawl_naver_news()
