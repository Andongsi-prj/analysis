import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import random
import pymysql
from tqdm import tqdm
import re

# âœ… ìµœê·¼ 7ì¼ê°„ì˜ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ë„ë¡ ì„¤ì •
start_date = datetime.now()
end_date = start_date - timedelta(days=6)  # ìµœê·¼ 7ì¼ê°„

# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL (âœ… 7ì¼ê°„ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ìœ„í•œ `nso=so:r,p:7d,a:all` ì¶”ê°€)
base_url = 'https://search.naver.com/search.naver?where=news&query=ì² ê°•&sm=tab_pge&sort=sim&ds={}&de={}&start={}&nso=so:r,p:7d,a:all'

# í˜ì´ì§€ ì œí•œ ì„¤ì •
max_pages = 5  # ğŸ”¹ í˜ì´ì§€ ì œí•œ (ë¶ˆí•„ìš”í•œ í¬ë¡¤ë§ ë°©ì§€, ì„±ëŠ¥ ìµœì í™”)

# âœ… User-Agent ëª©ë¡ (ëœë¤ ì„ íƒí•˜ì—¬ ì°¨ë‹¨ ë°©ì§€)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36",
]

# MySQL ì ‘ì† ì •ë³´ (`pymysql` ì‚¬ìš©)
DB_CONFIG = {
    "host": "192.168.0.163",  # MySQL ì„œë²„ ì£¼ì†Œ
    "user": "analysis_user",  # MySQL ì‚¬ìš©ì ì´ë¦„
    "password": "andong1234",  # MySQL ë¹„ë°€ë²ˆí˜¸
    "database": "analysis",  # ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ëª…
    "charset": "utf8mb4"  # UTF-8 ì§€ì›
}

# HTTP ìš”ì²­ ì„¸ì…˜ ì„¤ì • (ìë™ ì¬ì‹œë„ ì ìš©)
session = requests.Session()
retry_strategy = Retry(total=3, backoff_factor=2, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("https://", adapter)
session.mount("http://", adapter)

def connect_db():
    """MySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (`pymysql` ì‚¬ìš©)"""
    return pymysql.connect(**DB_CONFIG)

def insert_news_to_db(news_list):
    """í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ MySQL `stage_news` í…Œì´ë¸”ì— ì €ì¥ (ë‚ ì§œ í˜•ì‹ ìˆ˜ì •)"""
    if not news_list:
        print("âš ï¸ ì €ì¥í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    conn = connect_db()
    cursor = conn.cursor()

    sql = """
        INSERT INTO stage_news (title, content, news_date)
        VALUES (%s, %s, %s)
        ON DUPLICATE KEY UPDATE content=VALUES(content), news_date=VALUES(news_date)
    """
    
    formatted_news_list = []
    
    for title, content, news_date in news_list:
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYY-MM-DD ìœ ì§€)
        if news_date and news_date != "ì•Œ ìˆ˜ ì—†ìŒ":
            try:
                news_date = datetime.strptime(news_date, "%Y-%m-%d").date()
            except ValueError:
                news_date = None  # ë‚ ì§œ í˜•ì‹ì´ ë§ì§€ ì•Šìœ¼ë©´ NULL ì €ì¥
        else:
            news_date = None  # ì•Œ ìˆ˜ ì—†ëŠ” ë‚ ì§œëŠ” NULLë¡œ ì €ì¥

        formatted_news_list.append((title, content, news_date))

    try:
        cursor.executemany(sql, formatted_news_list)  # ğŸ”¹ ì—¬ëŸ¬ ê°œì˜ ë°ì´í„° í•œ ë²ˆì— ì‚½ì…
        conn.commit()  # ğŸ”¹ ë³€ê²½ ì‚¬í•­ ì ìš©
        
        print(f"âœ… {cursor.rowcount}ê°œì˜ ë‰´ìŠ¤ê°€ MySQL `stage_news` í…Œì´ë¸”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    except pymysql.MySQLError as e:
        print(f"âŒ MySQL ì €ì¥ ì˜¤ë¥˜: {e}")
    
    finally:
        cursor.close()
        conn.close()

def get_headers():
    """âœ… ëœë¤ User-Agent ì ìš©í•˜ì—¬ ì°¨ë‹¨ ë°©ì§€"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "https://www.naver.com/",
        "Accept-Language": "ko-KR,ko;q=0.9",
        "Connection": "keep-alive"
    }

def get_news_list(url, news_list, seen_links):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬"""
    try:
        time.sleep(random.uniform(1.5, 4))  # âœ… ëœë¤ ë”œë ˆì´ ì¶”ê°€ (ì°¨ë‹¨ ë°©ì§€)
        response = session.get(url, headers=get_headers(), timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"âŒ Error fetching URL {url}: {e}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.select(".news_area")

    for article in tqdm(articles, desc="í¬ë¡¤ë§ ì§„í–‰ì¤‘ (ì² ê°•)", ncols=80):
        try:
            title_element = article.select_one("a.news_tit")
            title = title_element.text.strip() if title_element else "ì œëª© ì—†ìŒ"
            link = title_element["href"] if title_element else ""

            # âœ… ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì´ë¯¸ ì €ì¥ëœ ë§í¬ëŠ” ê±´ë„ˆë›´ë‹¤
            if link in seen_links:
                continue
            seen_links.add(link)

            # ë³¸ë¬¸ ë‚´ìš©ê³¼ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
            content, news_date = get_article_content(link)

            # ìœ íš¨í•œ ë‰´ìŠ¤ì¸ì§€ í™•ì¸ í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if title != "ì œëª© ì—†ìŒ" and content != "ë‚´ìš© ì—†ìŒ" and len(content) > 50:
                news_list.append((title, content, news_date))

        except Exception as e:
            print(f"âŒ Error occurred: {e}")

def get_article_content(url, max_retries=3):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ê³¼ ë‚ ì§œë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    retries = 0
    while retries < max_retries:
        try:
            time.sleep(random.uniform(1.5, 4))  # âœ… ëœë¤ ë”œë ˆì´ ì¶”ê°€ (ì°¨ë‹¨ ë°©ì§€)
            response = session.get(url, headers=get_headers(), timeout=10)
            response.raise_for_status()
            break
        except requests.RequestException as e:
            print(f"âŒ Error fetching article {url}: {e}")
            retries += 1

    if retries == max_retries:
        return "ë‚´ìš© ì—†ìŒ", None  # âœ… news_dateê°€ Noneì´ë©´ MySQLì—ì„œ NULLë¡œ ì €ì¥ë¨

    soup = BeautifulSoup(response.text, "html.parser")

    # ğŸ”¹ ê´‘ê³  ë° ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
    for script in soup(["script", "style", "header", "footer", "nav"]):
        script.extract()

    # ğŸ”¹ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
    content_element = soup.select_one("#dic_area, .newsct_article, .article_body, .art_txt, div[itemprop='articleBody']")
    content = content_element.get_text(strip=True) if content_element else "ë‚´ìš© ì—†ìŒ"

    # ğŸ”¹ ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ
    news_date = None  
    date_element = soup.find("meta", {"property": "og:article:published_time"})
    if date_element:
        news_date = date_element["content"][:10]  

    if not news_date:
        possible_date_elements = soup.select(".media_end_head_info, .article_info, .date, time")
        for date_el in possible_date_elements:
            date_text = date_el.get_text(strip=True)
            match = re.search(r"\d{4}[./-]\d{2}[./-]\d{2}", date_text)
            if match:
                news_date = match.group().replace(".", "-")  
                break

    return content, news_date


if __name__ == "__main__":
    all_news = []
    seen_links = set()
    for single_date in range((start_date - end_date).days + 1):
        formatted_date = (start_date - timedelta(days=single_date)).strftime("%Y.%m.%d")
        for page in range(1, max_pages + 1):
            section_url = base_url.format(formatted_date, formatted_date, (page - 1) * 10 + 1)
            get_news_list(section_url, all_news, seen_links)

    if all_news:
        print("âœ… ìµœê·¼ 7ì¼ê°„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ.")
    else:
        print("âš ï¸ ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
