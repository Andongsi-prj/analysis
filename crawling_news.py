import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pymysql  # âœ… MySQL ì—°ê²°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re
import time

# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
base_url = "https://search.naver.com/search.naver?where=news&query=ì² ê°•&sm=tab_opt&sort=2&photo=0&field=0&pd=3&ds={}&de={}&start={}"

# âœ… MySQL ì—°ê²° ì„¤ì •
db = pymysql.connect(
    host="192.168.0.163",  # MySQL í˜¸ìŠ¤íŠ¸
    user="analysis_user",       # MySQL ì‚¬ìš©ì ì´ë¦„
    password="andong1234",  # MySQL ë¹„ë°€ë²ˆí˜¸
    database="analysis",   # ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
    charset="utf8mb4"
)

cursor = db.cursor()

# âœ… HTTP ì„¸ì…˜ ì„¤ì • ë° ì¬ì‹œë„ ì „ëµ ì ìš©
session = requests.Session()
retry_strategy = Retry(total=3, backoff_factor=2, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("https://", adapter)
session.mount("http://", adapter)

# âœ… ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ í—¤ë” ì„¤ì • (User-Agent ë° Cookie ì¶”ê°€)
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Referer": "https://www.naver.com/",
    "Accept-Language": "ko-KR,ko;q=0.9",
    "Cookie": "NNB=your_cookie_here;"  # ë„¤ì´ë²„ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¨ ì¿ í‚¤ ê°’ ì¶”ê°€
}

# âœ… ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ì œëª© ì €ì¥ (ì´ë¯¸ í¬ë¡¤ë§í•œ ê¸°ì‚¬ ì²´í¬)
seen_titles = set()

def get_news_list(url, counter, last_known_date):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‰´ìŠ¤ ì œëª©, ë³¸ë¬¸ ìš”ì•½, ë³´ë„ ë‚ ì§œ í¬ë¡¤ë§ í›„ MySQLì— ì €ì¥"""
    try:
        response = session.get(url, headers=headers)  # âœ… User-Agent ë° ì¿ í‚¤ ì¶”ê°€ëœ í—¤ë” ì‚¬ìš©
        if response.status_code == 403:
            print("âŒ 403 Forbidden - ë„¤ì´ë²„ ì°¨ë‹¨ë¨. ì¼ì • ì‹œê°„ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            time.sleep(600)  # âœ… 10ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„
            return False, last_known_date

        response.raise_for_status()
        time.sleep(3)  # âœ… ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ 3ì´ˆ ëŒ€ê¸°
    except requests.RequestException as e:
        print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
        return False, last_known_date  # âœ… íŠœí”Œ ë°˜í™˜

    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.select(".news_area")

    if not articles:
        return False, last_known_date  # âœ… íŠœí”Œ ë°˜í™˜

    for article in articles:
        try:
            # âœ… ê¸°ì‚¬ ì œëª© í¬ë¡¤ë§
            title_element = article.select_one("a.news_tit")
            title = title_element.text.strip() if title_element else "ì œëª© ì—†ìŒ"

            # âœ… ì¤‘ë³µ ê¸°ì‚¬ ë°©ì§€
            if title in seen_titles:
                continue
            seen_titles.add(title)

            # âœ… ë³´ë„ ë‚ ì§œ í¬ë¡¤ë§
            publish_date_element = article.select("span.info")
            publish_date = "ì•Œ ìˆ˜ ì—†ìŒ"
            for elem in publish_date_element:
                date_text = elem.get_text(strip=True)
                publish_date = parse_publish_date(date_text)
                break

            # âœ… ë‚ ì§œ í˜•ì‹ ë³€ê²½ (YYYY-MM-DD)
            if publish_date == "ì•Œ ìˆ˜ ì—†ìŒ":
                publish_date = last_known_date  # âœ… ì´ì „ ë‰´ìŠ¤ì˜ ë‚ ì§œë¡œ ì„¤ì •
            else:
                last_known_date = publish_date  # âœ… í˜„ì¬ ë‚ ì§œë¥¼ ì €ì¥í•˜ì—¬ ì´í›„ì— í™œìš©

            # âœ… ë³¸ë¬¸ ìš”ì•½ í¬ë¡¤ë§
            summary_element = article.select_one("div.news_contents > div > div > a")
            content = summary_element.text.strip() if summary_element else "ìš”ì•½ ì—†ìŒ"

            # âœ… ë°ì´í„° MySQLì— ì €ì¥
            save_to_db(title, content, publish_date)

            # âœ… ë‚ ì§œì™€ ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜ë§Œ ì¶œë ¥
            counter[0] += 1
            print(f"âœ… {publish_date} - ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜: {counter[0]}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return True, last_known_date  # âœ… íŠœí”Œ ë°˜í™˜

def parse_publish_date(date_text):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¨ ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ ë³€í™˜ (YYYY-MM-DD)"""
    current_time = datetime.now()

    match_day = re.search(r"(\d+)ì¼ ì „", date_text)
    if match_day:
        days_ago = int(match_day.group(1))
        return (current_time - timedelta(days=days_ago)).strftime("%Y-%m-%d")

    match_date = re.search(r"(\d{4})[.](\d{2})[.](\d{2})[.]", date_text)
    if match_date:
        return f"{match_date.group(1)}-{match_date.group(2)}-{match_date.group(3)}"

    return "ì•Œ ìˆ˜ ì—†ìŒ"

def save_to_db(title, content, news_date):
    """MySQLì— ë‰´ìŠ¤ ë°ì´í„° ì €ì¥"""
    try:
        insert_query = """
        INSERT INTO stage_news (title, content, news_date)
        VALUES (%s, %s, %s)
        """
        cursor.execute(insert_query, (title, content, news_date))
        db.commit()
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
        db.rollback()

if __name__ == "__main__":
    counter = [0] 
    last_known_date = None  # âœ… ì²˜ìŒì—ëŠ” ë‚ ì§œê°€ ì—†ìœ¼ë¯€ë¡œ Noneìœ¼ë¡œ ì„¤ì •

    # âœ… 12ì›” 6ì¼ë¶€í„° í¬ë¡¤ë§ ì‹œì‘
    start_date = datetime(2024, 12, 6)
    end_date = datetime(2025, 2, 12)

    while start_date <= end_date:
        # âœ… 3ì¼ ë‹¨ìœ„ ì„¤ì •
        next_date = start_date + timedelta(days=2)
        if next_date > end_date:
            next_date = end_date

        ds = start_date.strftime("%Y.%m.%d")
        de = next_date.strftime("%Y.%m.%d")

        print(f"ğŸ” {ds} ~ {de} ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")

        page = 1
        max_pages = 1000  

        while page <= max_pages:
            search_url = base_url.format(ds, de, (page - 1) * 10 + 1)
            prev_count = counter[0]
            success, last_known_date = get_news_list(search_url, counter, last_known_date)

            if not success or prev_count == counter[0]:  
                break
            page += 1  

        print(f"âœ… {ds} ~ {de} ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ! (ì´ ì €ì¥ëœ ê°œìˆ˜: {counter[0]})\n")

        # âœ… ë‹¤ìŒ 3ì¼ë¡œ ì´ë™
        start_date = next_date + timedelta(days=1)

    print("âœ… í¬ë¡¤ë§ ë° MySQL ì €ì¥ ì™„ë£Œ.")

    # âœ… MySQL ì—°ê²° ì¢…ë£Œ
    cursor.close()
    db.close()
