"""
ê³¼ê±°ë¡œ ë¬´í•œ í¬ë¡¤ë§
ì² ê°• ê²€ìƒ‰í•˜ë©´ ëœ¨ëŠ” ëª¨ë“  ê¸°ì‚¬
403 ë°œìƒ ì‹œ, 10ë¶„ í›„ ì¬ì‹œë„, ì‹¤íŒ¨ì‹œ í•´ë‹¹ ë‚ ì§œ ê¸°ë¡
ì¤‘ë³µ ê¸°ì‚¬ ë°©ì§€ : seen_titles ë° insert ignorea(ì¿¼ë¦¬)
"""


import random
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pymysql  # âœ… MySQL ë¼ì´ë¸ŒëŸ¬ë¦¬
import re
import time

# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
base_url = "https://search.naver.com/search.naver?where=news&query=ì² ê°•&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={}&de={}&start={}"

# âœ… MySQL ì—°ê²° ì„¤ì •
db = pymysql.connect(
    host="192.168.0.163",
    user="root",
    password="andong1234",
    database="analysis",
    charset="utf8mb4"
)
cursor = db.cursor()

# âœ… HTTP ì„¸ì…˜ ì„¤ì • ë° ì¬ì‹œë„ ì „ëµ ì ìš©
session = requests.Session()
retry_strategy = Retry(total=3, backoff_factor=2, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("https://", adapter)
session.mount("http://", adapter)

# âœ… ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ í—¤ë” ì„¤ì •
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Referer": "https://www.naver.com/",
    "Accept-Language": "ko-KR,ko;q=0.9",
}

# âœ… ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ì œëª© ì €ì¥ (ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤ ì²´í¬)
seen_titles = set()

# âœ… 403 ì—ëŸ¬ ë°œìƒ í›„ ì‹¤íŒ¨í•œ ë‚ ì§œ ì €ì¥ ë¦¬ìŠ¤íŠ¸
failed_dates = set()

def get_news_list(url, counter, current_date):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëª¨ë“  ê¸°ì‚¬ í¬ë¡¤ë§ í›„ MySQLì— ì €ì¥"""
    try:
        for attempt in range(2):  # âœ… 2ë²ˆê¹Œì§€ ì¬ì‹œë„ ê°€ëŠ¥
            response = session.get(url, headers=headers)
            if response.status_code == 403:
                if attempt == 0:
                    print(f"âŒ 403 Forbidden - {current_date} í¬ë¡¤ë§ ì°¨ë‹¨ë¨. 10ë¶„ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    time.sleep(600)  # âœ… 10ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    continue
                else:
                    print(f"âŒ {current_date} í¬ë¡¤ë§ ì¬ì‹œë„ ì‹¤íŒ¨. í•´ë‹¹ ë‚ ì§œë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.")
                    failed_dates.add(current_date)  # âœ… ì‹¤íŒ¨í•œ ë‚ ì§œ ê¸°ë¡
                    return False

            response.raise_for_status()
            random_sleep()
            break  # âœ… 403ì´ ë°œìƒí•˜ì§€ ì•Šìœ¼ë©´ ë£¨í”„ íƒˆì¶œ

    except requests.RequestException as e:
        print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
        return False

    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.select(".news_area")

    if not articles:
        return False

    for article in articles:
        try:
            # âœ… ê¸°ì‚¬ ì œëª© í¬ë¡¤ë§
            title_element = article.select_one("a.news_tit")
            title = title_element.text.strip() if title_element else "ì œëª© ì—†ìŒ"

            # âœ… ì¤‘ë³µ ê¸°ì‚¬ ë°©ì§€ (ì¤‘ë³µ ê¸°ì‚¬ë¼ë©´ ìŠ¤í‚µ)
            if title in seen_titles:
                continue
            seen_titles.add(title)

            # âœ… ë³´ë„ ë‚ ì§œ í¬ë¡¤ë§
            publish_date = current_date

            # âœ… ë³¸ë¬¸ ìš”ì•½ í¬ë¡¤ë§
            summary_element = article.select_one("a.dsc_txt_wrap")
            content = summary_element.text.strip() if summary_element else "ìš”ì•½ ì—†ìŒ"

            # âœ… ë°ì´í„° MySQLì— ì €ì¥
            save_to_db(title, content, publish_date)

            # âœ… ë‚ ì§œì™€ ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜ë§Œ ì¶œë ¥
            counter[0] += 1
            print(f"âœ… {publish_date} - ì €ì¥ëœ ë°ì´í„° ê°œìˆ˜: {counter[0]}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return True

def save_to_db(title, content, publish_date):
    """MySQLì— ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ (INSERT IGNORE ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)"""
    try:
        insert_query = """
        INSERT IGNORE INTO stage_news (title, content, news_date)
        VALUES (%s, %s, %s)
        """
        cursor.execute(insert_query, (title, content, publish_date))
        db.commit()
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
        db.rollback()

def random_sleep():
    """ëœë¤í•œ ì‹œê°„ ë™ì•ˆ ëŒ€ê¸° (3~4ì´ˆ, 10% í™•ë¥ ë¡œ 7~8.3ì´ˆ)"""
    sleep_time = random.uniform(3.5, 4)
    if random.random() < 0.1:
        sleep_time = random.uniform(7, 8.3)

    print(f"â³ {sleep_time:.2f}ì´ˆ ëŒ€ê¸°...")
    time.sleep(sleep_time)

if __name__ == "__main__":
    counter = [0]  
    start_date = datetime.now() - timedelta(days=1)  # âœ… ì–´ì œë¶€í„° ì‹œì‘

    try:
        while True:  # âœ… ì¢…ë£Œí•  ë•Œê¹Œì§€ ë¬´í•œ ë°˜ë³µ
            ds = start_date.strftime("%Y.%m.%d")
            de = ds  # âœ… í•˜ë£¨ ë‹¨ìœ„ í¬ë¡¤ë§

            print(f"ğŸ” {ds} ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")

            page = 1
            max_pages = 1000  

            while page <= max_pages:
                search_url = base_url.format(ds, de, (page - 1) * 10 + 1)
                prev_count = counter[0]
                success = get_news_list(search_url, counter, ds)

                if not success or prev_count == counter[0]:  
                    break
                page += 1  

            print(f"âœ… {ds} ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ! (ì´ ì €ì¥ëœ ê°œìˆ˜: {counter[0]})\n")

            # âœ… í•˜ë£¨ì”© ê³¼ê±°ë¡œ ì´ë™
            start_date -= timedelta(days=1)

    except KeyboardInterrupt:
        print("\nâ¹ í¬ë¡¤ë§ ì¤‘ì§€ ìš”ì²­ë¨. í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ë‚ ì§œ ë§ˆë¬´ë¦¬ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤...")
    finally:
        # âœ… í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ë‚ ì§œ ì¶œë ¥
        if failed_dates:
            print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ë‚ ì§œ ëª©ë¡:")
            for date in sorted(failed_dates):
                print(f"- {date}")

        print("âœ… í¬ë¡¤ë§ ë° MySQL ì €ì¥ ì™„ë£Œ.")
        cursor.close()
        db.close()
